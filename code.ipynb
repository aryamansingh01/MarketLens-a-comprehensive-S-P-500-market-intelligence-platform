{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PHASE 1: Stock Market Analysis Data Collection & Cleaning\n",
      "================================================================================\n",
      "\n",
      "[STEP 1] Downloading S&P 500 company list...\n",
      "✓ Downloaded 503 S&P 500 symbols\n",
      "\n",
      "[STEP 2] Downloading historical stock prices (5 years)...\n",
      "  This will take 15-25 minutes...\n",
      "  Progress: 50/503 (9.9%) - 0.3 min elapsed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1 Failed download:\n",
      "['BRK.B']: YFTzMissingError('possibly delisted; no timezone found')\n",
      "\n",
      "1 Failed download:\n",
      "['BF.B']: YFPricesMissingError('possibly delisted; no price data found  (1d 2020-11-12 18:14:12.398539 -> 2025-11-11 18:14:12.398539)')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 100/503 (19.9%) - 0.6 min elapsed\n",
      "  Progress: 150/503 (29.8%) - 0.9 min elapsed\n",
      "  Progress: 200/503 (39.8%) - 1.2 min elapsed\n",
      "  Progress: 250/503 (49.7%) - 1.5 min elapsed\n",
      "  Progress: 300/503 (59.6%) - 1.8 min elapsed\n",
      "  Progress: 350/503 (69.6%) - 2.1 min elapsed\n",
      "  Progress: 400/503 (79.5%) - 2.5 min elapsed\n",
      "  Progress: 450/503 (89.5%) - 2.7 min elapsed\n",
      "  Progress: 500/503 (99.4%) - 3.1 min elapsed\n",
      "\n",
      "✓ Stock prices saved (621931 records)\n",
      "  Downloaded: 501 symbols\n",
      "  Failed: 2 symbols\n",
      "\n",
      "[STEP 3] Downloading fundamentals (this is slow - downloading 100 companies)...\n",
      "  Tip: Increase limit by modifying symbols_to_download list\n",
      "  500/503 companies downloaded...\n",
      "✓ Fundamentals saved (503 records)\n",
      "  Failed: 0 companies\n",
      "\n",
      "[STEP 4] Downloading macroeconomic indicators...\n",
      "  ✓ US_10Yr_Bond_Yield\n",
      "  ✓ CPI_All_Urban_Consumers\n",
      "  ✓ Unemployment_Rate\n",
      "  ✓ Industrial_Production_Index\n",
      "  ✓ Term_Spread\n",
      "  ✓ VIX\n",
      "✓ Macro data saved (1317 records)\n",
      "\n",
      "[STEP 5] Downloading sector ETF performance...\n",
      "  ✓ XLK (Technology)\n",
      "  ✓ XLV (Healthcare)\n",
      "  ✓ XLF (Finance)\n",
      "  ✓ XLE (Energy)\n",
      "  ✓ XLI (Industrials)\n",
      "  ✓ XLY (Consumer_Discretionary)\n",
      "  ✓ XLP (Consumer_Staples)\n",
      "  ✓ XLRE (Real_Estate)\n",
      "  ✓ XLU (Utilities)\n",
      "  ✓ SPY (S&P_500_Index)\n",
      "✓ Sector ETF data saved\n",
      "\n",
      "[STEP 6] Cleaning and finalizing data for Power BI...\n",
      "✓ Stock prices cleaned and saved\n",
      "✓ Fundamentals cleaned and saved\n",
      "✓ Macro indicators cleaned and saved\n",
      "✓ Sector ETFs cleaned and saved\n",
      "\n",
      "================================================================================\n",
      "✓ DATA COLLECTION COMPLETE!\n",
      "================================================================================\n",
      "Total time: 6.5 minutes\n",
      "\n",
      "Cleaned files ready in: data/cleaned/\n",
      "  1. stock_prices_cleaned.csv\n",
      "  2. fundamentals_cleaned.csv\n",
      "  3. macro_indicators_cleaned.csv (optional)\n",
      "  4. sector_etfs_cleaned.csv (optional)\n",
      "\n",
      "Import the CSV files into Power BI Desktop!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from pandas_datareader import data as pdr\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import requests\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "# Suppress FutureWarning about auto_adjust\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PHASE 1: Stock Market Analysis Data Collection & Cleaning\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create data directories\n",
    "os.makedirs('data', exist_ok=True)\n",
    "os.makedirs('data/cleaned', exist_ok=True)\n",
    "\n",
    "failed_prices = []\n",
    "failed_fund = []\n",
    "start_time = datetime.now()\n",
    "\n",
    "# ---- STEP 1: Get S&P 500 company list ----\n",
    "print(\"\\n[STEP 1] Downloading S&P 500 company list...\")\n",
    "try:\n",
    "    url = 'https://en.wikipedia.org/wiki/List_of_S%26P_500_companies'\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}\n",
    "    response = requests.get(url, headers=headers, timeout=10)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    tables = pd.read_html(response.text)\n",
    "    sp500 = None\n",
    "    for table in tables:\n",
    "        if 'Symbol' in table.columns:\n",
    "            sp500 = table\n",
    "            break\n",
    "\n",
    "    if sp500 is not None:\n",
    "        symbols = sp500['Symbol'].tolist()\n",
    "        sp500.to_csv('data/sp500_companies.csv', index=False)\n",
    "        with open('data/sp500_symbols.txt', 'w') as f:\n",
    "            for symbol in symbols:\n",
    "                f.write(symbol + '\\n')\n",
    "        print(f\"✓ Downloaded {len(symbols)} S&P 500 symbols\")\n",
    "    else:\n",
    "        raise Exception(\"Could not find Symbol column in Wikipedia tables\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error in Step 1: {str(e)}\")\n",
    "    print(\"  Attempting fallback...\")\n",
    "    try:\n",
    "        tables = pd.read_html(url)\n",
    "        sp500 = tables[0]\n",
    "        symbols = sp500['Symbol'].tolist()\n",
    "        sp500.to_csv('data/sp500_companies.csv', index=False)\n",
    "        with open('data/sp500_symbols.txt', 'w') as f:\n",
    "            for symbol in symbols:\n",
    "                f.write(symbol + '\\n')\n",
    "        print(f\"✓ Fallback successful: {len(symbols)} symbols\")\n",
    "    except Exception as e2:\n",
    "        print(f\"✗ Fallback failed: {str(e2)}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "# ---- STEP 2: Download historical stock prices ----\n",
    "print(\"\\n[STEP 2] Downloading historical stock prices (5 years)...\")\n",
    "print(\"  This will take 15-25 minutes...\")\n",
    "\n",
    "end_date = datetime.now()\n",
    "start_date = end_date - timedelta(days=5*365)\n",
    "stock_data = pd.DataFrame()\n",
    "\n",
    "for i, symbol in enumerate(symbols):\n",
    "    try:\n",
    "        # Download with auto_adjust=True for adjusted prices\n",
    "        df = yf.download(symbol, start=start_date, end=end_date, progress=False, timeout=10, auto_adjust=True)\n",
    "        \n",
    "        if df.empty:\n",
    "            failed_prices.append(symbol)\n",
    "            continue\n",
    "        \n",
    "        # Flatten MultiIndex columns if they exist\n",
    "        if isinstance(df.columns, pd.MultiIndex):\n",
    "            df.columns = df.columns.get_level_values(0)\n",
    "        \n",
    "        # Ensure 'Adj Close' exists, else create from 'Close'\n",
    "        if 'Adj Close' not in df.columns and 'Close' in df.columns:\n",
    "            df['Adj Close'] = df['Close']\n",
    "        \n",
    "        df['Symbol'] = symbol\n",
    "        df['Date'] = df.index\n",
    "        stock_data = pd.concat([stock_data, df], ignore_index=False)\n",
    "    except Exception as e:\n",
    "        failed_prices.append(symbol)\n",
    "        print(f\"  ✗ Failed to download {symbol}: {str(e)[:50]}\")\n",
    "\n",
    "    if (i + 1) % 50 == 0:\n",
    "        elapsed = (datetime.now() - start_time).total_seconds() / 60\n",
    "        print(f\"  Progress: {i+1}/{len(symbols)} ({(i+1)/len(symbols)*100:.1f}%) - {elapsed:.1f} min elapsed\")\n",
    "\n",
    "if not stock_data.empty:\n",
    "    stock_data.reset_index(drop=True, inplace=True)\n",
    "    available_cols = ['Date', 'Symbol', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']\n",
    "    cols_to_select = [col for col in available_cols if col in stock_data.columns]\n",
    "    stock_data = stock_data[cols_to_select]\n",
    "    stock_data.sort_values(['Symbol', 'Date'], inplace=True, ignore_index=True)\n",
    "    stock_data.to_csv('data/sp500_stock_prices_5years.csv', index=False)\n",
    "    print(f\"\\n✓ Stock prices saved ({len(stock_data)} records)\")\n",
    "    print(f\"  Downloaded: {len(symbols) - len(failed_prices)} symbols\")\n",
    "    print(f\"  Failed: {len(failed_prices)} symbols\")\n",
    "else:\n",
    "    print(\"✗ No stock price data collected\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# ---- STEP 3: Download financial fundamentals (top 100 companies for speed) ----\n",
    "print(\"\\n[STEP 3] Downloading fundamentals (this is slow - downloading 100 companies)...\")\n",
    "print(\"  Tip: Increase limit by modifying symbols_to_download list\")\n",
    "\n",
    "symbols_to_download = symbols\n",
    "\n",
    "fundamentals = []\n",
    "for i, symbol in enumerate(symbols_to_download):\n",
    "    try:\n",
    "        ticker = yf.Ticker(symbol)\n",
    "        info = ticker.info\n",
    "        fundamentals.append({\n",
    "            'Symbol': symbol,\n",
    "            'Company_Name': info.get('longName', 'N/A'),\n",
    "            'Sector': info.get('sector', 'N/A'),\n",
    "            'Industry': info.get('industry', 'N/A'),\n",
    "            'Market_Cap': info.get('marketCap'),\n",
    "            'P_E_Ratio': info.get('trailingPE'),\n",
    "            'Dividend_Yield': info.get('dividendYield'),\n",
    "            'EPS': info.get('trailingEps'),\n",
    "            'Revenue': info.get('totalRevenue'),\n",
    "            'Profit_Margin': info.get('profitMargins'),\n",
    "            'Debt_to_Equity': info.get('debtToEquity'),\n",
    "            'Current_Ratio': info.get('currentRatio'),\n",
    "            'ROE': info.get('returnOnEquity'),\n",
    "            'ROA': info.get('returnOnAssets'),\n",
    "            'Beta': info.get('beta'),\n",
    "            '52_Week_High': info.get('fiftyTwoWeekHigh'),\n",
    "            '52_Week_Low': info.get('fiftyTwoWeekLow'),\n",
    "            'Average_Volume': info.get('averageVolume'),\n",
    "            'Shares_Outstanding': info.get('sharesOutstanding'),\n",
    "        })\n",
    "    except Exception as e:\n",
    "        failed_fund.append((symbol, str(e)[:50]))\n",
    "    if (i + 1) % 10 == 0:\n",
    "        print(f\"  {i+1}/{len(symbols_to_download)} companies downloaded...\", end='\\r')\n",
    "        time.sleep(0.5)\n",
    "\n",
    "fundamentals_df = pd.DataFrame(fundamentals)\n",
    "\n",
    "if not fundamentals_df.empty:\n",
    "    fundamentals_df.to_csv('data/sp500_fundamentals.csv', index=False)\n",
    "    print(f\"\\n✓ Fundamentals saved ({len(fundamentals_df)} records)\")\n",
    "    print(f\"  Failed: {len(failed_fund)} companies\")\n",
    "else:\n",
    "    print(\"\\n✗ No fundamentals data collected\")\n",
    "\n",
    "# ---- STEP 4: Download macroeconomic indicators ----\n",
    "print(\"\\n[STEP 4] Downloading macroeconomic indicators...\")\n",
    "\n",
    "macro_codes = {\n",
    "    'DGS10': 'US_10Yr_Bond_Yield',\n",
    "    'CPIAUCSL': 'CPI_All_Urban_Consumers',\n",
    "    'UNRATE': 'Unemployment_Rate',\n",
    "    'INDPRO': 'Industrial_Production_Index',\n",
    "    'T10Y2Y': 'Term_Spread',\n",
    "    'VIXCLS': 'VIX',\n",
    "}\n",
    "\n",
    "macro_data = pd.DataFrame()\n",
    "for code, name in macro_codes.items():\n",
    "    try:\n",
    "        d_fred = pdr.get_data_fred(code, start=start_date, end=end_date)\n",
    "        if not d_fred.empty:\n",
    "            d_fred.rename(columns={code: name}, inplace=True)\n",
    "            d_fred = d_fred.reset_index()\n",
    "            if 'Date' not in d_fred.columns:\n",
    "                d_fred.rename(columns={d_fred.columns[0]: 'Date'}, inplace=True)\n",
    "            if macro_data.empty:\n",
    "                macro_data = d_fred\n",
    "            else:\n",
    "                macro_data = pd.merge(macro_data, d_fred, on='Date', how='outer')\n",
    "            print(f\"  ✓ {name}\")\n",
    "        else:\n",
    "            print(f\"  ✗ No data for {name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Failed {code}: {str(e)[:50]}\")\n",
    "\n",
    "if not macro_data.empty:\n",
    "    if 'Date' in macro_data.columns:\n",
    "        macro_data.sort_values('Date', inplace=True)\n",
    "        macro_data.fillna(method='ffill', inplace=True)\n",
    "        macro_data.to_csv('data/macroeconomic_indicators.csv', index=False)\n",
    "        print(f\"✓ Macro data saved ({len(macro_data)} records)\")\n",
    "    else:\n",
    "        print(\"⚠️ 'Date' column missing in macroeconomic data — skipping save\")\n",
    "else:\n",
    "    print(\"⚠️ No macroeconomic data collected\")\n",
    "\n",
    "# ---- STEP 5: Download sector ETF data ----\n",
    "print(\"\\n[STEP 5] Downloading sector ETF performance...\")\n",
    "\n",
    "sector_etfs = {\n",
    "    'XLK': 'Technology',\n",
    "    'XLV': 'Healthcare',\n",
    "    'XLF': 'Finance',\n",
    "    'XLE': 'Energy',\n",
    "    'XLI': 'Industrials',\n",
    "    'XLY': 'Consumer_Discretionary',\n",
    "    'XLP': 'Consumer_Staples',\n",
    "    'XLRE': 'Real_Estate',\n",
    "    'XLU': 'Utilities',\n",
    "    'SPY': 'S&P_500_Index',\n",
    "}\n",
    "\n",
    "sector_data = pd.DataFrame()\n",
    "for ticker, name in sector_etfs.items():\n",
    "    try:\n",
    "        data = yf.download(ticker, start=start_date, end=end_date, progress=False, timeout=10, auto_adjust=True)\n",
    "        if not data.empty:\n",
    "            if isinstance(data.columns, pd.MultiIndex):\n",
    "                data.columns = data.columns.get_level_values(0)\n",
    "            df = pd.DataFrame({\n",
    "                'Date': data.index,\n",
    "                'Ticker': ticker,\n",
    "                'Sector': name,\n",
    "                'Open': data['Open'].values,\n",
    "                'High': data['High'].values,\n",
    "                'Low': data['Low'].values,\n",
    "                'Close': data['Close'].values,\n",
    "                'Volume': data['Volume'].values,\n",
    "            })\n",
    "            sector_data = pd.concat([sector_data, df], ignore_index=False)\n",
    "            print(f\"  ✓ {ticker} ({name})\")\n",
    "        else:\n",
    "            print(f\"  ✗ No data for {ticker}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Failed {ticker}: {str(e)[:50]}\")\n",
    "\n",
    "if not sector_data.empty:\n",
    "    sector_data.reset_index(drop=True, inplace=True)\n",
    "    sector_data.sort_values(['Ticker', 'Date'], inplace=True)\n",
    "    sector_data.to_csv('data/sector_etf_performance.csv', index=False)\n",
    "    print(\"✓ Sector ETF data saved\")\n",
    "else:\n",
    "    print(\"⚠️ No sector ETF data collected\")\n",
    "\n",
    "# ---- STEP 6: Data cleaning ----\n",
    "print(\"\\n[STEP 6] Cleaning and finalizing data for Power BI...\")\n",
    "\n",
    "if os.path.exists('data/sp500_stock_prices_5years.csv'):\n",
    "    try:\n",
    "        stock_prices = pd.read_csv('data/sp500_stock_prices_5years.csv')\n",
    "        stock_prices['Date'] = pd.to_datetime(stock_prices['Date'])\n",
    "        stock_prices = stock_prices[stock_prices['Close'] > 0]\n",
    "        stock_prices = stock_prices[stock_prices['High'] >= stock_prices['Low']]\n",
    "        stock_prices.sort_values(['Symbol', 'Date'], inplace=True)\n",
    "        stock_prices['Daily_Return'] = stock_prices.groupby('Symbol')['Adj Close'].pct_change()\n",
    "        stock_prices.to_csv('data/cleaned/stock_prices_cleaned.csv', index=False)\n",
    "        print(\"✓ Stock prices cleaned and saved\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error cleaning stock prices: {str(e)}\")\n",
    "\n",
    "if os.path.exists('data/sp500_fundamentals.csv'):\n",
    "    try:\n",
    "        fundamentals = pd.read_csv('data/sp500_fundamentals.csv')\n",
    "        numeric_columns = ['Market_Cap', 'P_E_Ratio', 'Dividend_Yield', 'EPS', 'Revenue',\n",
    "                          'Profit_Margin', 'Debt_to_Equity', 'Current_Ratio', 'ROE', 'ROA',\n",
    "                          'Beta', '52_Week_High', '52_Week_Low', 'Average_Volume', 'Shares_Outstanding']\n",
    "        for col in numeric_columns:\n",
    "            fundamentals[col] = pd.to_numeric(fundamentals[col], errors='coerce')\n",
    "        def categorize_market_cap(market_cap):\n",
    "            if pd.isna(market_cap):\n",
    "                return 'Unknown'\n",
    "            elif market_cap >= 10e9:\n",
    "                return 'Large Cap'\n",
    "            elif market_cap >= 2e9:\n",
    "                return 'Mid Cap'\n",
    "            else:\n",
    "                return 'Small Cap'\n",
    "        fundamentals['Market_Cap_Category'] = fundamentals['Market_Cap'].apply(categorize_market_cap)\n",
    "        fundamentals.to_csv('data/cleaned/fundamentals_cleaned.csv', index=False)\n",
    "        print(\"✓ Fundamentals cleaned and saved\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error cleaning fundamentals: {str(e)}\")\n",
    "\n",
    "if os.path.exists('data/macroeconomic_indicators.csv'):\n",
    "    try:\n",
    "        macro_indicators = pd.read_csv('data/macroeconomic_indicators.csv')\n",
    "        macro_indicators['Date'] = pd.to_datetime(macro_indicators['Date'])\n",
    "        macro_indicators.fillna(method='ffill', inplace=True)\n",
    "        macro_indicators.to_csv('data/cleaned/macro_indicators_cleaned.csv', index=False)\n",
    "        print(\"✓ Macro indicators cleaned and saved\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error cleaning macro indicators: {str(e)}\")\n",
    "\n",
    "if os.path.exists('data/sector_etf_performance.csv'):\n",
    "    try:\n",
    "        sector_etfs_data = pd.read_csv('data/sector_etf_performance.csv')\n",
    "        sector_etfs_data['Date'] = pd.to_datetime(sector_etfs_data['Date'])\n",
    "        sector_etfs_data.sort_values(['Ticker', 'Date'], inplace=True)\n",
    "        sector_etfs_data.to_csv('data/cleaned/sector_etfs_cleaned.csv', index=False)\n",
    "        print(\"✓ Sector ETFs cleaned and saved\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error cleaning sector ETFs: {str(e)}\")\n",
    "\n",
    "elapsed_total = (datetime.now() - start_time).total_seconds() / 60\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"✓ DATA COLLECTION COMPLETE!\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Total time: {elapsed_total:.1f} minutes\")\n",
    "print(\"\\nCleaned files ready in: data/cleaned/\")\n",
    "print(\"  1. stock_prices_cleaned.csv\")\n",
    "print(\"  2. fundamentals_cleaned.csv\")\n",
    "print(\"  3. macro_indicators_cleaned.csv (optional)\")\n",
    "print(\"  4. sector_etfs_cleaned.csv (optional)\")\n",
    "print(\"\\nImport the CSV files into Power BI Desktop!\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
